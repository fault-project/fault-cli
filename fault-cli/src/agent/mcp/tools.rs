use std::collections::HashMap;
use std::fs;
use std::path::Path;
use std::str::FromStr;
use std::sync::Arc;

use rmcp::{
    // Errors + macros + wrappers
    ErrorData as McpError,
    handler::server::wrapper::{Json, Parameters},
    model::{CallToolResult, Content, ServerCapabilities, ServerInfo},
    schemars::JsonSchema,
    serde::{Deserialize, Serialize},
    serde_json::{self, Value, json},
    tool,
    tool_handler,
    tool_router,
};
use similar::TextDiff;
use swiftide::indexing::EmbeddedField;
use swiftide::integrations::fastembed::FastEmbed;
use swiftide::integrations::qdrant::Qdrant;
use swiftide::query;
use swiftide::query::answers;
use swiftide::query::query_transformers;
use swiftide_core::EmbeddingModel;
use swiftide_core::SimplePrompt;
use url::Url;

use crate::agent::CODE_COLLECTION;
use crate::agent::clients::SupportedLLMClient;
use crate::agent::clients::get_client;
use crate::agent::mcp::code;
use crate::agent::mcp::code::extract_function_snippet;
use crate::agent::mcp::code::guess_file_language;
use crate::agent::mcp::code::list_functions;
use crate::report;
use crate::report::types::Report;
use crate::scenario::executor::run_scenario_first_item;
use crate::scenario::types::Scenario;
use crate::scenario::types::ScenarioItem;
use crate::scenario::types::ScenarioItemCall;
use crate::scenario::types::ScenarioItemCallStrategy;
use crate::scenario::types::ScenarioItemContext;
use crate::scenario::types::ScenarioItemProxySettings;
use crate::scenario::types::ScenarioItemSLO;
use crate::types::BandwidthUnit;
use crate::types::FaultConfiguration;
use crate::types::StreamSide;

// -----------------------------------------------------------------------------
// Helpers
// -----------------------------------------------------------------------------

fn strip_file_scheme(s: &str) -> &str {
    s.strip_prefix("file://").unwrap_or(s)
}

fn mcp_internal(ctx: &'static str, e: impl ToString) -> McpError {
    McpError::internal_error(
        ctx,
        Some(serde_json::json!({ "err": e.to_string() })),
    )
}

fn mcp_invalid(ctx: &'static str, extra: Value) -> McpError {
    McpError::invalid_params(ctx, Some(extra))
}

// -----------------------------------------------------------------------------
// Payloads (shared)
// -----------------------------------------------------------------------------

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
pub struct CodeBlock {
    #[schemars(
        description = "Full function block (signature + body + decorators)"
    )]
    pub full: String,
    #[schemars(description = "Function body only")]
    pub body: String,
}

#[derive(Debug, Serialize, Deserialize, Default, JsonSchema)]
pub struct CodeChange {
    #[schemars(description = "Score before changes (optional)")]
    pub score: f64,
    #[schemars(description = "Short summary of threats & changes")]
    pub explanation: String,
    #[schemars(description = "Full content before")]
    pub old: String,
    #[schemars(description = "Full content after")]
    pub new: String,
    #[schemars(description = "Dependencies to install (if any)")]
    pub dependencies: Vec<String>,
    #[schemars(description = "Unified diff between old and new")]
    pub diff: String,
}

// -----------------------------------------------------------------------------
// MCP server struct
// -----------------------------------------------------------------------------

#[derive(Clone)]
pub struct FaultMCP {
    pub llm_type: SupportedLLMClient,
    pub prompt_model: String,
    pub embed_model: String,
    pub embed_model_dim: u64,

    // router (generated by #[tool_router] impl below)
    tool_router: rmcp::handler::server::router::tool::ToolRouter<FaultMCP>,
}

impl FaultMCP {
    pub fn new(
        llm_type: SupportedLLMClient,
        prompt_model: &str,
        embed_model: &str,
        embed_model_dim: u64,
    ) -> Self {
        Self {
            llm_type,
            prompt_model: prompt_model.into(),
            embed_model: embed_model.into(),
            embed_model_dim,
            tool_router: Self::tool_router(),
        }
    }
}

// -----------------------------------------------------------------------------
// Tool params / results (per tool)
// -----------------------------------------------------------------------------

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct ListFnParams {
    #[schemars(description = "Absolute path; file:// accepted")]
    file: String,
}
#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct ListFnResult {
    names: Vec<String>,
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct ExtractBlockParams {
    #[schemars(description = "Absolute path; file:// accepted")]
    file: String,
    #[schemars(description = "Function name to extract")]
    func: String,
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct IndexSourceParams {
    #[schemars(description = "Directory to index; file:// accepted")]
    source_dir: String,
    #[schemars(description = "Language: rust, python, go...")]
    lang: String,
}
#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct DoneResult {
    status: String,
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct ScoreParams {
    #[schemars(description = "Code block / snippet")]
    snippet: String,
    #[schemars(description = "Language")]
    lang: String,
}
#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct ScoreResult {
    #[schemars(description = "Raw model JSON output")]
    value: Value,
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct SuggestFnParams {
    #[schemars(description = "Code block / snippet")]
    snippet: String,
    #[schemars(description = "Language")]
    lang: String,
    #[schemars(description = "Current score 0..1")]
    score: f64,
    #[schemars(description = "Target score 0..1")]
    target_score: f64,
}
#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct DiffResult {
    diff: String,
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct FullChangesParams {
    #[schemars(description = "Absolute path; file:// accepted")]
    file: String,
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct SloParams {
    #[schemars(description = "Code block / snippet")]
    snippet: String,
    #[schemars(description = "Language")]
    lang: String,
}
#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct TextResult {
    text: String,
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct ScenarioLatencyParams {
    url: String,
    method: String,
    body: String,
    duration: String,
    latency: f64,
    deviation: f64,
    direction: String, // ingress|egress
    side: String,      // client|server
    per_read_write_op: bool,
    num_clients: usize,
    rps: usize,
    timeout: u64,
    proxies: Vec<String>,
}
#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct ScenarioBasicParams {
    url: String,
    method: String,
    body: String,
    duration: String,
    num_clients: usize,
    rps: usize,
    timeout: u64,
    proxies: Vec<String>,

    // variant-specific
    // Jitter
    #[serde(default)]
    amplitude: f64,
    #[serde(default)]
    frequency: f64,
    // Bandwidth
    #[serde(default)]
    rate: u32,
    #[serde(default)]
    unit: String,
    // Direction & side
    #[serde(default)]
    direction: String,
    #[serde(default)]
    side: String,
    // HTTP error
    #[serde(default)]
    probability: f64,
    #[serde(default)]
    status_code: u16,
    #[serde(default)]
    error_body: String,
}
#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct ScenarioResult {
    report: String,
}

#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct DeepAnalysisParams {
    #[schemars(description = "File absolute path; file:// accepted")]
    file: String,
    lang: String,
    func: String,
    #[schemars(description = "Subset: performance|reliability|threat")]
    concerns: Vec<String>,
}
#[derive(Debug, Serialize, Deserialize, JsonSchema)]
struct DeepAnalysisResult {
    evaluations: Vec<String>,
    prompts: Vec<String>,
}

// -----------------------------------------------------------------------------
// Tool router: all tools live here, each takes Parameters<T> and returns
// Json<T>
// -----------------------------------------------------------------------------

const PERF_TPL: &str =
    include_str!("../prompts/tool_score_code_performance.md");
const REL_TPL: &str = include_str!("../prompts/tool_score_code_reliability.md");
const SLO_TPL: &str = include_str!("../prompts/tool_suggest_slo.md");
const SUGG_PERF_TPL: &str =
    include_str!("../prompts/tool_suggest_perf_improvement.md");
const SUGG_REL_TPL: &str =
    include_str!("../prompts/tool_suggest_reliability_improvement.md");
const FULL_CHANGESET_TPL: &str =
    include_str!("../prompts/tool_suggest_complete_reliability_changeset.md");

#[tool_router]
impl FaultMCP {
    // -------- Code navigation --------

    #[tool(
        name = "fault_list_function_names",
        description = "List all function names in a source file"
    )]
    async fn fault_list_function_names(
        &self,
        params: Parameters<ListFnParams>,
    ) -> Result<Json<ListFnResult>, McpError> {
        let path = strip_file_scheme(&params.0.file);
        let src = fs::read_to_string(path)
            .map_err(|e| mcp_internal("file_read", e))?;

        let ext = Path::new(path)
            .extension()
            .and_then(|s| s.to_str())
            .ok_or_else(|| mcp_invalid("bad_uri", json!({ "file": path })))?;

        let names = list_functions(&src, ext).ok_or_else(|| {
            mcp_invalid("func_not_found", json!({ "file": path }))
        })?;

        Ok(Json(ListFnResult { names }))
    }

    #[tool(
        name = "fault_extract_code_block",
        description = "Extract function code block by name"
    )]
    async fn fault_extract_code_block(
        &self,
        params: Parameters<ExtractBlockParams>,
    ) -> Result<Json<CodeBlock>, McpError> {
        let path = strip_file_scheme(&params.0.file);
        let src = fs::read_to_string(path)
            .map_err(|e| mcp_internal("file_read", e))?;

        let ext = Path::new(path)
            .extension()
            .and_then(|s| s.to_str())
            .ok_or_else(|| mcp_invalid("bad_uri", json!({ "file": path })))?;

        let snippet = extract_function_snippet(&src, ext, &params.0.func)
            .ok_or_else(|| {
                mcp_invalid("func_not_found", json!({ "func": params.0.func }))
            })?;

        Ok(Json(CodeBlock { full: snippet.full, body: snippet.body }))
    }

    #[tool(
        name = "fault_index_source_code",
        description = "Index a source code directory"
    )]
    async fn fault_index_source_code(
        &self,
        params: Parameters<IndexSourceParams>,
    ) -> Result<Json<DoneResult>, McpError> {
        code::index(
            strip_file_scheme(&params.0.source_dir),
            &params.0.lang,
            "/tmp/index.db",
            self.llm_type,
            &self.prompt_model,
            &self.embed_model,
            self.embed_model_dim,
        )
        .await
        .map_err(|e| mcp_internal("code_index", e))?;

        Ok(Json(DoneResult { status: "done".into() }))
    }

    // -------- Scoring --------

    #[tool(
        name = "fault_score_performance",
        description = "Compute a performance score for a code snippet"
    )]
    async fn fault_score_performance(
        &self,
        params: Parameters<ScoreParams>,
    ) -> Result<Json<ScoreResult>, McpError> {
        let prompt = PERF_TPL
            .replace("{lang}", &params.0.lang)
            .replace("{snippet}", &params.0.snippet);

        let llm =
            get_client(self.llm_type, &self.prompt_model, &self.embed_model)
                .map_err(|e| mcp_internal("client_build", e))?;

        let answer = llm
            .prompt(prompt.into())
            .await
            .map_err(|e| mcp_internal("llm_prompt", e))?;
        let parsed: Value = serde_json::from_str(&answer)
            .map_err(|e| mcp_internal("parse_score_response", e))?;

        Ok(Json(ScoreResult { value: parsed }))
    }

    #[tool(
        name = "fault_score_reliability",
        description = "Compute a reliability score for a code snippet"
    )]
    async fn fault_score_reliability(
        &self,
        params: Parameters<ScoreParams>,
    ) -> Result<Json<ScoreResult>, McpError> {
        let prompt = REL_TPL
            .replace("{lang}", &params.0.lang)
            .replace("{snippet}", &params.0.snippet);

        let llm =
            get_client(self.llm_type, &self.prompt_model, &self.embed_model)
                .map_err(|e| mcp_internal("client_build", e))?;

        let answer = llm
            .prompt(prompt.into())
            .await
            .map_err(|e| mcp_internal("llm_prompt", e))?;
        let parsed: Value = serde_json::from_str(&answer)
            .map_err(|e| mcp_internal("parse_score_response", e))?;

        Ok(Json(ScoreResult { value: parsed }))
    }

    // -------- Suggestions (diffs) --------

    #[tool(
        name = "fault_suggest_better_function_performance",
        description = "Generate a unified diff to improve performance"
    )]
    async fn fault_suggest_better_function_performance(
        &self,
        params: Parameters<SuggestFnParams>,
    ) -> Result<Json<DiffResult>, McpError> {
        let prompt = SUGG_PERF_TPL
            .replace("{lang}", &params.0.lang)
            .replace("{snippet}", &params.0.snippet)
            .replace("{score}", &params.0.score.to_string())
            .replace("{target_score}", &params.0.target_score.to_string());

        let llm =
            get_client(self.llm_type, &self.prompt_model, &self.embed_model)
                .map_err(|e| mcp_internal("client_build", e))?;
        let sp: Arc<dyn SimplePrompt> = llm.clone();
        let em: Arc<dyn EmbeddingModel> = llm.clone();

        let qdrant: Qdrant = Qdrant::builder()
            .batch_size(50)
            .vector_size(self.embed_model_dim)
            .with_vector(EmbeddedField::Combined)
            .with_sparse_vector(EmbeddedField::Combined)
            .collection_name(CODE_COLLECTION)
            .build()
            .map_err(|e| mcp_internal("qdrant_builder", e))?;

        let pipeline = query::Pipeline::default()
            .then_transform_query(query_transformers::Embed::from_client(
                em.clone(),
            ))
            .then_retrieve(qdrant.clone())
            .then_answer(answers::Simple::from_client(sp.clone()));

        let resp = pipeline
            .query(prompt)
            .await
            .map_err(|e| mcp_internal("query", e))?;
        Ok(Json(DiffResult { diff: resp.answer().to_string() }))
    }

    #[tool(
        name = "fault_suggest_better_function_reliability",
        description = "Generate a unified diff to improve reliability"
    )]
    async fn fault_suggest_better_function_reliability(
        &self,
        params: Parameters<SuggestFnParams>,
    ) -> Result<Json<DiffResult>, McpError> {
        let prompt = SUGG_REL_TPL
            .replace("{lang}", &params.0.lang)
            .replace("{snippet}", &params.0.snippet)
            .replace("{score}", &params.0.score.to_string())
            .replace("{target_score}", &params.0.target_score.to_string());

        let llm =
            get_client(self.llm_type, &self.prompt_model, &self.embed_model)
                .map_err(|e| mcp_internal("client_build", e))?;
        let sp: Arc<dyn SimplePrompt> = llm.clone();
        let em: Arc<dyn EmbeddingModel> = llm.clone();

        let qdrant: Qdrant = Qdrant::builder()
            .batch_size(50)
            .vector_size(self.embed_model_dim)
            .with_vector(EmbeddedField::Combined)
            .with_sparse_vector(EmbeddedField::Combined)
            .collection_name(CODE_COLLECTION)
            .build()
            .map_err(|e| mcp_internal("qdrant_builder", e))?;

        let pipeline = query::Pipeline::default()
            .then_transform_query(query_transformers::Embed::from_client(
                em.clone(),
            ))
            .then_retrieve(qdrant.clone())
            .then_answer(answers::Simple::from_client(sp.clone()));

        let resp = pipeline
            .query(prompt)
            .await
            .map_err(|e| mcp_internal("query", e))?;
        Ok(Json(DiffResult { diff: resp.answer().to_string() }))
    }

    #[tool(
        name = "fault_make_reliability_and_perf_changes",
        description = "Generate a unified diff patch for a source file"
    )]
    async fn fault_make_reliability_and_perf_changes(
        &self,
        params: Parameters<FullChangesParams>,
    ) -> Result<Json<CodeChange>, McpError> {
        let path = strip_file_scheme(&params.0.file);
        let lang = guess_file_language(path)
            .map_err(|e| mcp_internal("guess_file_language", e))?;
        let snippet = fs::read_to_string(path)
            .map_err(|e| mcp_internal("read_file", e))?;

        let prompt = FULL_CHANGESET_TPL
            .replace("{lang}", &lang)
            .replace("{snippet}", &snippet);

        let llm =
            get_client(self.llm_type, &self.prompt_model, &self.embed_model)
                .map_err(|e| mcp_internal("client_build", e))?;
        let answer = llm
            .prompt(prompt.into())
            .await
            .map_err(|e| mcp_internal("llm_prompt", e))?;

        let mut parsed = CodeChange::default();
        if let Some(code) = code::extract_json_fence(&answer) {
            parsed = serde_json::from_str(&code)
                .map_err(|e| mcp_internal("parse_code_change", e))?;

            let filename = Path::new(path)
                .file_name()
                .and_then(|os| os.to_str())
                .unwrap_or("file");
            parsed.diff = TextDiff::from_lines(&snippet, &parsed.new)
                .unified_diff()
                .context_radius(10)
                .header(filename, filename)
                .to_string();
        }

        Ok(Json(parsed))
    }

    #[tool(
        name = "fault_suggest_service_level_objectives_slo",
        description = "Suggest SLOs for a code snippet"
    )]
    async fn fault_suggest_service_level_objectives_slo(
        &self,
        params: Parameters<SloParams>,
    ) -> Result<Json<TextResult>, McpError> {
        let prompt = SLO_TPL
            .replace("{lang}", &params.0.lang)
            .replace("{snippet}", &params.0.snippet);

        let llm =
            get_client(self.llm_type, &self.prompt_model, &self.embed_model)
                .map_err(|e| mcp_internal("client_build", e))?;
        let answer = llm
            .prompt(prompt.into())
            .await
            .map_err(|e| mcp_internal("llm_prompt", e))?;

        Ok(Json(TextResult { text: answer }))
    }

    // -------- Scenarios --------

    #[tool(
        name = "fault_run_latency_impact_scenario",
        description = "Measure impact of increased latency on response time"
    )]
    async fn fault_run_latency_impact_scenario(
        &self,
        params: Parameters<ScenarioLatencyParams>,
    ) -> Result<Json<ScenarioResult>, McpError> {
        let stream_side = if params.0.side == "client" {
            StreamSide::Client
        } else {
            StreamSide::Server
        };
        let fault = FaultConfiguration::Latency {
            distribution: Some("normal".to_string()),
            global: Some(!params.0.per_read_write_op),
            side: Some(stream_side),
            mean: Some(params.0.latency),
            stddev: Some(params.0.deviation),
            min: None,
            max: None,
            shape: None,
            scale: None,
            direction: Some(params.0.direction),
            period: None,
        };

        let report = run_scenario(
            params.0.url,
            params.0.method,
            params.0.body,
            params.0.duration,
            fault,
            params.0.num_clients,
            params.0.rps,
            params.0.timeout,
            params.0.proxies,
        )
        .await?;

        Ok(Json(ScenarioResult { report: report.render() }))
    }

    #[tool(
        name = "fault_run_jitter_impact_scenario",
        description = "Measure impact of jitter on response time"
    )]
    async fn fault_run_jitter_impact_scenario(
        &self,
        params: Parameters<ScenarioBasicParams>,
    ) -> Result<Json<ScenarioResult>, McpError> {
        let stream_side = if params.0.side == "client" {
            StreamSide::Client
        } else {
            StreamSide::Server
        };
        let fault = FaultConfiguration::Jitter {
            amplitude: params.0.amplitude,
            frequency: params.0.frequency,
            direction: Some(params.0.direction),
            side: Some(stream_side),
            period: None,
        };

        let report = run_scenario(
            params.0.url,
            params.0.method,
            params.0.body,
            params.0.duration,
            fault,
            params.0.num_clients,
            params.0.rps,
            params.0.timeout,
            params.0.proxies,
        )
        .await?;

        Ok(Json(ScenarioResult { report: report.render() }))
    }

    #[tool(
        name = "fault_run_bandwidth_impact_scenario",
        description = "Measure impact of bandwidth constraints"
    )]
    async fn fault_run_bandwidth_impact_scenario(
        &self,
        params: Parameters<ScenarioBasicParams>,
    ) -> Result<Json<ScenarioResult>, McpError> {
        let stream_side = if params.0.side == "client" {
            StreamSide::Client
        } else {
            StreamSide::Server
        };
        let fault = FaultConfiguration::Bandwidth {
            rate: params.0.rate,
            unit: BandwidthUnit::from_str(&params.0.unit).unwrap_or_default(),
            direction: Some(params.0.direction),
            side: Some(stream_side),
            period: None,
        };

        let report = run_scenario(
            params.0.url,
            params.0.method,
            params.0.body,
            params.0.duration,
            fault,
            params.0.num_clients,
            params.0.rps,
            params.0.timeout,
            params.0.proxies,
        )
        .await?;

        Ok(Json(ScenarioResult { report: report.render() }))
    }

    #[tool(
        name = "fault_run_packet_loss_impact_scenario",
        description = "Measure impact of packet loss"
    )]
    async fn fault_run_packet_loss_impact_scenario(
        &self,
        params: Parameters<ScenarioBasicParams>,
    ) -> Result<Json<ScenarioResult>, McpError> {
        let stream_side = if params.0.side == "client" {
            StreamSide::Client
        } else {
            StreamSide::Server
        };
        let fault = FaultConfiguration::PacketLoss {
            direction: Some(params.0.direction),
            side: Some(stream_side),
            period: None,
        };

        let report = run_scenario(
            params.0.url,
            params.0.method,
            params.0.body,
            params.0.duration,
            fault,
            params.0.num_clients,
            params.0.rps,
            params.0.timeout,
            params.0.proxies,
        )
        .await?;

        Ok(Json(ScenarioResult { report: report.render() }))
    }

    #[tool(
        name = "fault_run_http_error_impact_scenario",
        description = "Measure impact of HTTP errors"
    )]
    async fn fault_run_http_error_impact_scenario(
        &self,
        params: Parameters<ScenarioBasicParams>,
    ) -> Result<Json<ScenarioResult>, McpError> {
        let fault = FaultConfiguration::HttpError {
            status_code: params.0.status_code,
            body: Some(params.0.error_body),
            probability: params.0.probability,
            period: None,
        };

        let report = run_scenario(
            params.0.url,
            params.0.method,
            params.0.body,
            params.0.duration,
            fault,
            params.0.num_clients,
            params.0.rps,
            params.0.timeout,
            params.0.proxies,
        )
        .await?;

        Ok(Json(ScenarioResult { report: report.render() }))
    }

    #[tool(
        name = "fault_run_blackhole_impact_scenario",
        description = "Measure impact of a network blackhole"
    )]
    async fn fault_run_blackhole_impact_scenario(
        &self,
        params: Parameters<ScenarioBasicParams>,
    ) -> Result<Json<ScenarioResult>, McpError> {
        let stream_side = if params.0.side == "client" {
            StreamSide::Client
        } else {
            StreamSide::Server
        };
        let fault = FaultConfiguration::Blackhole {
            direction: Some(params.0.direction),
            side: Some(stream_side),
            period: None,
        };

        let report = run_scenario(
            params.0.url,
            params.0.method,
            params.0.body,
            params.0.duration,
            fault,
            params.0.num_clients,
            params.0.rps,
            params.0.timeout,
            params.0.proxies,
        )
        .await?;

        Ok(Json(ScenarioResult { report: report.render() }))
    }

    #[tool(
        name = "fault_function_reliability_deep_analysis",
        description = "Deep evaluation of a function (reliability/perf/threat)"
    )]
    async fn fault_function_reliability_deep_analysis(
        &self,
        params: Parameters<DeepAnalysisParams>,
    ) -> Result<Json<DeepAnalysisResult>, McpError> {
        let path = strip_file_scheme(&params.0.file);
        let src = fs::read_to_string(path)
            .map_err(|e| mcp_internal("file_read", e))?;

        let ext = Path::new(path)
            .extension()
            .and_then(|s| s.to_str())
            .ok_or_else(|| mcp_internal("file_ext_not_found", path))?;

        let snippet = extract_function_snippet(&src, ext, &params.0.func)
            .ok_or_else(|| {
                mcp_invalid("func_not_found", json!({ "func": params.0.func }))
            })?;

        let llm =
            get_client(self.llm_type, &self.prompt_model, &self.embed_model)
                .map_err(|e| mcp_internal("client_build", e))?;

        let templates = vec![
            (
                "performance",
                include_str!("../prompts/tool_eval_code_performance.md"),
            ),
            (
                "reliability",
                include_str!("../prompts/tool_eval_code_reliability.md"),
            ),
            ("threat", include_str!("../prompts/tool_eval_code_scenario.md")),
        ];

        let mut evaluations = Vec::new();
        let mut prompts = Vec::new();

        for (angle, tmpl) in templates {
            if !params.0.concerns.is_empty()
                && !params.0.concerns.iter().any(|c| c == angle)
            {
                continue;
            }
            let prompt = tmpl
                .replace("{file}", path)
                .replace("{func}", &params.0.func)
                .replace("{snippet}", &snippet.full)
                .replace("{lang}", &params.0.lang);
            let answer = llm
                .prompt(prompt.clone().into())
                .await
                .map_err(|e| mcp_internal("llm_prompt", e))?;
            prompts.push(prompt);
            evaluations.push(answer);
        }

        Ok(Json(DeepAnalysisResult { evaluations, prompts }))
    }
}

// -----------------------------------------------------------------------------
// Hook the router into the server
// -----------------------------------------------------------------------------

#[tool_handler]
impl rmcp::ServerHandler for FaultMCP {
    fn get_info(&self) -> ServerInfo {
        ServerInfo {
            instructions: Some("Fault â€” reliability & performance MCP.".into()),
            capabilities: ServerCapabilities::builder().enable_tools().build(),
            ..Default::default()
        }
    }
}

// -----------------------------------------------------------------------------
// Scenario driver (rmcp-friendly)
// -----------------------------------------------------------------------------

async fn run_scenario(
    url: String,
    method: String,
    body: String,
    duration: String,
    fault: FaultConfiguration,
    num_clients: usize,
    rps: usize,
    timeout: u64,
    proxies: Vec<String>,
) -> Result<Report, McpError> {
    let components =
        Url::parse(&url).map_err(|e| mcp_internal("parse_url", e))?;
    let scheme = components.scheme();
    let host = components
        .host_str()
        .ok_or_else(|| mcp_internal("extract_url_host", "missing host"))?;
    let port = components
        .port_or_known_default()
        .ok_or_else(|| mcp_internal("extract_url_port", "missing port"))?;
    let origin = format!("{}://{}:{}", scheme, host, port);

    let headers = if !body.is_empty() {
        let mut h = HashMap::<String, String>::new();
        let _ =
            h.insert("content-type".to_owned(), "application/json".to_owned());
        Some(h)
    } else {
        None
    };

    let mut disable_http_proxies = false;
    let mut proxies_mapping = Vec::<String>::new();
    if !proxies.is_empty() {
        disable_http_proxies = true;
        proxies_mapping.extend(proxies);
    } else {
        proxies_mapping.push(format!("{}={}", 3180, origin));
    }

    let s = Scenario {
        title: format!("Evaluating runtime performance of {}", url),
        description: None,
        items: vec![ScenarioItem {
            call: ScenarioItemCall {
                method,
                url,
                headers,
                body: (!body.is_empty()).then(|| body),
                timeout: Some(timeout * 1000u64),
                meta: None,
            },
            context: ScenarioItemContext {
                upstreams: vec![],
                faults: vec![fault],
                strategy: Some(ScenarioItemCallStrategy::Load {
                    duration,
                    clients: num_clients,
                    rps,
                }),
                slo: Some(vec![
                    ScenarioItemSLO {
                        slo_type: "latency".into(),
                        title: "99% @ 350ms".into(),
                        objective: 99.0,
                        threshold: 350.0,
                    },
                    ScenarioItemSLO {
                        slo_type: "latency".into(),
                        title: "95% @ 200ms".into(),
                        objective: 95.0,
                        threshold: 200.0,
                    },
                ]),
                proxy: Some(ScenarioItemProxySettings {
                    disable_http_proxies,
                    proxies: proxies_mapping,
                }),
                runs_on: None,
            },
            expect: None,
        }],
        config: None,
    };

    let results = run_scenario_first_item(s)
        .await
        .map_err(|e| mcp_internal("run_scenario", e))?;
    Ok(report::builder::to_report(&results))
}
